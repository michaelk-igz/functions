{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream to Parquet Logger\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "\n",
    "python -m pip install pandas\n",
    "python -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.build.baseImage to 'mlrun/ml-models'\n",
      "%nuclio: setting spec.readinessTimeoutSeconds to 200\n",
      "%nuclio: setting kind to 'nuclio'\n"
     ]
    }
   ],
   "source": [
    "%%nuclio config\n",
    "spec.build.baseImage = \"mlrun/ml-models\"\n",
    "spec.readinessTimeoutSeconds = 200\n",
    "kind = \"nuclio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    setattr(context, 'batch', [])\n",
    "    setattr(context, 'batch_size', int(os.getenv('BATCH_SIZE', 1024)))\n",
    "\n",
    "    setattr(context, 'timestamp_key', os.getenv('TS_KEY'))\n",
    "    setattr(context, 'timestamp_format', os.getenv('TS_FORMAT', '%Y-%m-%d %H:%M:%S.%f'))\n",
    "\n",
    "    setattr(context, 'pq_partitions', ['pq_year', 'pq_month', 'pq_day', 'pq_hour'])\n",
    "\n",
    "    setattr(context, 'target_path', os.getenv('TARGET_PATH'))\n",
    "    os.makedirs(context.target_path, exist_ok=True)\n",
    "\n",
    "    # in case of an inference stream set the names of features and predictions.\n",
    "    features = os.getenv('FEATURES')\n",
    "    if features is not None:\n",
    "        features = features.split(',')\n",
    "    setattr(context, 'features', features)\n",
    "\n",
    "    predictions = os.getenv('PREDICTIONS')\n",
    "    if predictions is not None:\n",
    "        predictions = predictions.split(',')\n",
    "    setattr(context, 'predictions', predictions)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    if type(event.body) is dict:\n",
    "        event_dict = event.body\n",
    "    else:\n",
    "        event_dict = json.loads(event.body)\n",
    "\n",
    "    context.logger.info_with('Got invoked',\n",
    "                             trigger_kind=event.trigger.kind,\n",
    "                             event_body=event_dict)\n",
    "\n",
    "    # for inference events\n",
    "    if context.features is not None and context.predictions is not None:\n",
    "        event_dict = flatten_inference_event(context, event_dict)\n",
    "\n",
    "    event_with_time_partitions = add_time_partition_attributes(context, event_dict)\n",
    "\n",
    "    # add the incoming event to the current batch\n",
    "    context.batch.append(event_with_time_partitions)\n",
    "\n",
    "    # check if batch size reached\n",
    "    if context.batch_size == len(context.batch):\n",
    "        written_records = write_batch(context)\n",
    "        context.logger.info_with('Written batch',\n",
    "                                 Writtent_records=written_records)\n",
    "    pass\n",
    "\n",
    "\n",
    "def flatten_inference_event(context, event):\n",
    "    # add parsed features to the event\n",
    "    feature_values = event['request']['instances'][0]\n",
    "    event.update(zip(context.features, feature_values))\n",
    "\n",
    "    # add parsed predictions to the event\n",
    "    prediction_values = event['resp']\n",
    "    event.update(zip(context.predictions, prediction_values))\n",
    "    \n",
    "    return event\n",
    "\n",
    "\n",
    "def add_time_partition_attributes(context, event):\n",
    "    if hasattr(context, 'timestamp_key') and event.get(context.timestamp_key) is not None:\n",
    "        # parse the event time\n",
    "        dt_object = datetime.strptime(event[context.timestamp_key], context.timestamp_format)\n",
    "    else:\n",
    "        # if event time is missing or not configured, use current datetime\n",
    "        dt_object = datetime.now()\n",
    "\n",
    "    # add the partition attributes\n",
    "    event['pq_year'] = dt_object.strftime('%Y')\n",
    "    event['pq_month'] = dt_object.strftime('%m')\n",
    "    event['pq_day'] = dt_object.strftime('%d')\n",
    "    event['pq_hour'] = dt_object.strftime('%H')\n",
    "\n",
    "    return event\n",
    "\n",
    "\n",
    "def write_batch(context):\n",
    "    df = pd.DataFrame.from_records(context.batch)\n",
    "    df.to_parquet(path=context.target_path, partition_cols=context.pq_partitions)\n",
    "    # post write cleanup\n",
    "    context.batch = []\n",
    "    return len(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to function yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-08 15:37:49,211 [info] function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7fe98864ef90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import code_to_function, mount_v3io\n",
    "\n",
    "fn = code_to_function(name='stream-to-parquet-logger')\n",
    "\n",
    "# add metadata (for templates and reuse)\n",
    "fn.spec.default_handler = \"handler\"\n",
    "fn.spec.description = \"Saves a stream to Parquet log\"\n",
    "fn.metadata.categories = [\"serve\", \"stream\"]\n",
    "fn.metadata.labels = {\"author\": \"michaelk\"}\n",
    "fn.export(\"function.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7fe98864ef90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set few parameters via environment variables0\n",
    "fn.set_envs({'TARGET_PATH' : '/User/path/to/parquet/log',\n",
    "             'BATCH_SIZE': 1024,\n",
    "             'TS_KEY': 'event_time',\n",
    "             'TS_FORMAT': '%Y-%m-%d %H:%M:%S.%f',\n",
    "             'FEATURES': 'feat1,feat2,feat3', #optional for inference stream\n",
    "             'PREDICTIONS': 'pred1,pred2' #optional for inference stream\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7fe98864ef90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.add_trigger('stream2pq',\n",
    "                      nuclio.triggers.V3IOStreamTrigger(url='http://v3io-webapi:8081/users/path/to/stream@consumergroup',\n",
    "                                                        access_key=os.getenv('V3IO_ACCESS_KEY'),\n",
    "                                                        maxWorkers=10))\n",
    "\n",
    "# Configure a mount on the nuclio function from '/User' to our home directory '~/'.\n",
    "fn.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-08 15:38:31,537 [info] deploy started\n",
      "[nuclio] 2020-10-08 15:38:31,555 project name not found created new (my-project)\n",
      "[nuclio] 2020-10-08 15:38:36,665 (info) Build complete\n",
      "[nuclio] 2020-10-08 15:41:10,166 done creating my-project-stream-to-parquet-logger, function address: 192.168.226.12:30221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://192.168.226.12:30221'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.deploy(project='my-project')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
